{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2bfa141",
   "metadata": {},
   "source": [
    "# 一、 特征可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97357db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from src.models.video_model import VideoCompressor\n",
    "from skimage import io\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model = VideoCompressor()\n",
    "new_state_dict = torch.load('output/codec-10-01_14-34-31/epoch_0000_step008000.pt', map_location='cpu')\n",
    "new_state_dict_ = {}\n",
    "for name,param in new_state_dict.items():\n",
    "    new_state_dict_[name.replace('module.', '')] = param\n",
    "model.load_state_dict(new_state_dict_, strict=True)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "rate_idx = 3\n",
    "p_frame_y_q_scales = [1.2383, 0.9623, 0.7134, 0.5319]\n",
    "p_frame_mv_y_q_scales = [1.1844, 1.1044, 1.0107, 0.9189]\n",
    "\n",
    "def to_tensor(img_path):\n",
    "    img = io.imread(img_path)\n",
    "    img = torch.from_numpy(img).float().permute(2, 0, 1)[None, ] / 255.\n",
    "    return img\n",
    "\n",
    "def PSNR(input1, input2):\n",
    "    mse = torch.mean((input1 - input2) ** 2)\n",
    "    psnr = 20 * torch.log10(1 / torch.sqrt(mse))\n",
    "    return psnr.item()\n",
    "\n",
    "\n",
    "ref_image = to_tensor('/home/ziming_wang/code/UVG/CropVideos/Beauty/Beauty/im032.png').to(device)\n",
    "input_image = to_tensor('/home/ziming_wang/code/UVG/CropVideos/Beauty/Beauty/im033.png').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2519ace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpb = {\n",
    "        \"ref_frame\": ref_image,\n",
    "        \"ref_feature\": None,\n",
    "        \"ref_y\": None,\n",
    "        \"ref_mv_y\": None,\n",
    "    }\n",
    "x = input_image\n",
    "mv_y_q_scale = p_frame_mv_y_q_scales[rate_idx]\n",
    "y_q_scale = p_frame_y_q_scales[rate_idx]\n",
    "curr_mv_y_q = model.get_curr_mv_y_q(mv_y_q_scale)\n",
    "curr_y_q = model.get_curr_y_q(y_q_scale)\n",
    "\n",
    "est_mv = model.optic_flow(x, dpb[\"ref_frame\"])\n",
    "mv_y = model.mv_encoder(est_mv)\n",
    "mv_y = mv_y / curr_mv_y_q\n",
    "mv_z = model.mv_hyper_prior_encoder(mv_y)\n",
    "mv_z_hat = model.quant(mv_z)\n",
    "mv_params = model.mv_hyper_prior_decoder(mv_z_hat)\n",
    "ref_mv_y = dpb[\"ref_mv_y\"]\n",
    "if ref_mv_y is None:\n",
    "    ref_mv_y = torch.zeros_like(mv_y)\n",
    "mv_params = torch.cat((mv_params, ref_mv_y), dim=1)\n",
    "mv_q_step, mv_scales, mv_means = model.mv_y_prior_fusion(mv_params).chunk(3, 1)\n",
    "mv_y_res, mv_y_q, mv_y_hat, mv_scales_hat = model.forward_prior(\n",
    "    mv_y, mv_means, mv_scales, mv_q_step, model.mv_y_spatial_prior)\n",
    "mv_y_hat = mv_y_hat * curr_mv_y_q\n",
    "\n",
    "mv_hat = model.mv_decoder(mv_y_hat)\n",
    "context1, context2, context3, warp_frame = model.motion_compensation(dpb, mv_hat)\n",
    "\n",
    "y = model.contextual_encoder(x, context1, context2, context3)\n",
    "y = y / curr_y_q\n",
    "z = model.contextual_hyper_prior_encoder(y)\n",
    "z_hat = model.quant(z)\n",
    "hierarchical_params = model.contextual_hyper_prior_decoder(z_hat)\n",
    "temporal_params = model.temporal_prior_encoder(context3)\n",
    "\n",
    "ref_y = dpb[\"ref_y\"]\n",
    "if ref_y is None:\n",
    "    ref_y = torch.zeros_like(y)\n",
    "params = torch.cat((temporal_params, hierarchical_params, ref_y), dim=1)\n",
    "q_step, scales, means = model.y_prior_fusion(params).chunk(3, 1)\n",
    "y_res, y_q, y_hat, scales_hat = model.forward_prior(\n",
    "    y, means, scales, q_step, model.y_spatial_prior)\n",
    "y_hat = y_hat * curr_y_q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b53011",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_ = torch.zeros_like(y_hat)\n",
    "# y_hat_[0, 52] = y_hat[0, 52]\n",
    "context1_ = torch.zeros_like(context1)\n",
    "context2_ = torch.zeros_like(context2)\n",
    "context3_ = torch.zeros_like(context3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b299000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_image_feature = model.contextual_decoder(y_hat, context2, context3)\n",
    "feature, recon_image = model.recon_generation_net(recon_image_feature, context1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc13aab9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from src.utils.flowlib import flow_to_image\n",
    "flow_img, _ = flow_to_image(mv_hat[0].detach().cpu().numpy())\n",
    "plt.imshow(recon_image[0].detach().permute(1, 2, 0).cpu().numpy())\n",
    "plt.show()\n",
    "plt.imshow(flow_img)\n",
    "PSNR(recon_image, input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01747e97",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_hat.abs().square().sum(dim=-1).sum(dim=-1)[0].sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba0df92",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x1 = y_res.detach().numpy()\n",
    "plt.clf()\n",
    "for i in range(x1.shape[1]):\n",
    "    plt.figure(i)\n",
    "    t = x1[0][i]\n",
    "    t = (t - t.min()) / (t.max() - t.min())\n",
    "#     plt.matshow(t, aspect='equal', cmap='gray')\n",
    "    plt.colorbar()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d7262a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62a8c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(list(range(64)), filter_attention[0, :, 0, 0].detach().numpy())\n",
    "plt.imshow(spatial_attention[0,0,0,0].detach().numpy())\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e965ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dev.scale_hyper_prior import ScaleHyperprior\n",
    "import torch\n",
    "from skimage import io\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def PSNR(input1, input2):\n",
    "    mse = torch.mean((input1 - input2) ** 2)\n",
    "    psnr = 20 * torch.log10(1 / torch.sqrt(mse))\n",
    "    return psnr.item()\n",
    "# ckpt_path = \"/data/lqh/.cache/torch/hub/checkpoints/bmshj2018-hyperprior-8-a583f0cf.pth.tar\"\n",
    "ckpt_path = \"scale_hyper_prior.pth\"\n",
    "state_dict = torch.load(ckpt_path, map_location='cpu')\n",
    "model = ScaleHyperprior.from_state_dict(state_dict)\n",
    "model.eval()\n",
    "img = io.imread(\"kodim23.png\")[None,]\n",
    "img_ = io.imread(\"kodim23_.png\")[None,]\n",
    "imgs = np.concatenate([img, img_], axis=0)\n",
    "imgs = torch.from_numpy(imgs).permute(0, 3, 1, 2) / 255.\n",
    "# res = model.compress(imgs)\n",
    "# res = model.decompress(res['strings'], res['shape'])\n",
    "# x_hat =res['x_hat']\n",
    "# psnr = PSNR(x_hat, img)\n",
    "# torchvision.utils.save_image(x_hat[0], \"kodim23_.png\")\n",
    "# print(\"PSNR: {:.2f}\".format(psnr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863c4a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model.g_a(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19f3a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "hist = torch.histc(y, min=-4, max=4, bins=512)\n",
    "plt.plot(np.arange(-4, 4, 8 / 512.), hist.detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d826da",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = y.round()\n",
    "plt.clf()\n",
    "hist = torch.histc(y_, min=-4, max=4, bins=512)\n",
    "plt.plot(np.arange(-4, 4, 8 / 512.), hist.detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8a085c",
   "metadata": {},
   "source": [
    "# 二、 初始化权重模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9b537d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.models.video_model import VideoCompressor\n",
    "\n",
    "model = VideoCompressor(use_sal=True)\n",
    "# torch.save(model.state_dict(), \"checkpoints/offset_model_init.pth.tar\")\n",
    "# new_state_dict = torch.load('output/codec-09-24_11-22-24/epoch_0000_step002000.pt', map_location='cpu')\n",
    "# state_dict = torch.load('checkpoints/acmmm2022_video_psnr.pth.tar', map_location='cpu')\n",
    "# new_state_dict_ = {}\n",
    "# for name,param in new_state_dict.items():\n",
    "#     new_state_dict_[name.replace('module.', '')] = param\n",
    "# for name, param in new_state_dict_.items():\n",
    "#     if name in state_dict and param.shape == state_dict[name].shape:\n",
    "#         new_state_dict_[name] = state_dict[name]\n",
    "#     else:\n",
    "#         new_state_dict_[name] = param\n",
    "\n",
    "# model.load_state_dict(new_state_dict_, strict=True)\n",
    "\n",
    "state_dict = torch.load('./codec/output/variable-rate-mask/epoch_0006.pt', map_location='cpu')\n",
    "state_dict_ = {}\n",
    "for name,param in state_dict.items():\n",
    "    state_dict_[name.replace('module.', '')] = param\n",
    "\n",
    "new_state_dict = {}\n",
    "for name, param in model.named_parameters():\n",
    "    if name in state_dict_ and param.shape == state_dict_[name].shape:\n",
    "        new_state_dict[name] = state_dict_[name]\n",
    "    else:\n",
    "        new_state_dict[name] = param\n",
    "        print('not found', name)\n",
    "torch.save(new_state_dict, \"checkpoints_init.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e59178",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if 'contextual_condition_prior' in name:\n",
    "        tar_name = name.replace('condition', 'hyper')\n",
    "        if param.shape == state_dict_[tar_name].shape:\n",
    "            new_state_dict[name] = state_dict_[tar_name]\n",
    "            print('Copy from ', tar_name, 'to', name)\n",
    "        else:\n",
    "            print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a3cf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(new_state_dict, \"cdconv_checkpoints_init.pth.tar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95429147",
   "metadata": {},
   "source": [
    "# 三、灰度共生矩阵等纹理可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416eb329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from codec.src.dataset.fast_glcm import gen_features\n",
    "import imageio.v2 as imageio\n",
    "from codec.src.dataset.fast_glcm import gen_feature\n",
    "\n",
    "img = imageio.imread('/data/lqh/deepvideo/SalCodec/kodim23.png')\n",
    "R = img[:, :, 0]\n",
    "res = gen_feature(R)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.clf()\n",
    "plt.imshow(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5c9320",
   "metadata": {},
   "source": [
    "# 四、UVG Test Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d880030",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def cal_metric(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    bpp = defaultdict()\n",
    "    psnr = defaultdict()\n",
    "    missim = defaultdict()\n",
    "    for ds, results in data.items():\n",
    "        for seq_name, seq in results.items():\n",
    "            for rate_num, res in seq.items():\n",
    "                bpp.setdefault(rate_num, []).append(res['ave_all_frame_bpp'])\n",
    "                psnr.setdefault(rate_num, []).append(res['ave_all_frame_psnr'])\n",
    "                missim.setdefault(rate_num, []).append(res['ave_all_frame_msssim'])\n",
    "    res = defaultdict()\n",
    "    for rate_num in bpp.keys():\n",
    "        res.setdefault('bpp', []).append(np.mean(bpp[rate_num]))\n",
    "        res.setdefault('psnr', []).append(np.mean(psnr[rate_num]))\n",
    "        res.setdefault('msssim', []).append(np.mean(missim[rate_num]))\n",
    "    for key, value in res.items():\n",
    "        res[key] = np.array(value)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92043f19",
   "metadata": {},
   "source": [
    "### 单个视频指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a0a86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = '/data/lqh/deepvideo/SalCodec/codec/metric/baseline.json'\n",
    "with open(json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "for ds, results in data.items():\n",
    "    for seq_name, info in results.items():\n",
    "        bpp, psnr, msssim = [], [], []\n",
    "        for rate_num, res in info.items():\n",
    "            bpp.append(res['ave_all_frame_bpp'])\n",
    "            psnr.append(res['ave_all_frame_psnr'])\n",
    "            msssim.append(res['ave_all_frame_msssim'])\n",
    "        plt.plot(bpp, psnr, label=seq_name)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d811a107",
   "metadata": {},
   "source": [
    "## 4.1 添加offset部分权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee8711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = cal_metric('/data/lqh/deepvideo/SalCodec/codec/metric/baseline.json')\n",
    "new_model = cal_metric('/data/lqh/deepvideo/SalCodec/codec/2step1.json')\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(baseline['bpp'], baseline['psnr'], label='baseline')\n",
    "plt.plot(new_model['bpp'], new_model['psnr'], label='new_model')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516f1863",
   "metadata": {},
   "source": [
    "### 交叉weight，减去offset bpp\n",
    "从曲线上观察，如果offset不算进bpp里面，添加offset还是能轻微提升模型效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f90fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = cal_metric('/data/lqh/deepvideo/SalCodec/codec/metric/baseline.json')\n",
    "new_model = cal_metric('/data/lqh/deepvideo/SalCodec/codec/metric/cross_weight_no_offset.json')\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(baseline['bpp'], baseline['psnr'], label='baseline')\n",
    "plt.plot(new_model['bpp'], new_model['psnr'], label='new_model')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1548ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = cal_metric('/data/lqh/deepvideo/SalCodec/codec/metric/baseline.json')\n",
    "new_model = cal_metric('/data/lqh/deepvideo/SalCodec/codec/metric/multi_frame_e1s1w.json')\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(baseline['bpp'], baseline['psnr'], label='baseline')\n",
    "plt.plot(new_model['bpp'], new_model['psnr'], label='new_model')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4738076f",
   "metadata": {},
   "source": [
    "# 可视化重建帧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70d22c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "org_img = io.imread('/data/lqh/deepvideo/DVC/DVC/data/UVG/CropVideos/ShakeNDry/ShakeNDry/im001.png')\n",
    "dec_img = io.imread('/data/lqh/deepvideo/DVC/DVC/data/UVG/CropVideos/Bosphorus/Bosphorus/im001.png')\n",
    "h, w = org_img.shape[:-1]\n",
    "plt.imshow(dec_img)\n",
    "plt.show()\n",
    "psnr = PSNR(org_img, dec_img)\n",
    "gray = rgb2gray(dec_img)\n",
    "plt.imshow(gray, 'gray')\n",
    "plt.show()\n",
    "f = np.fft.fft2(gray)\n",
    "f = np.fft.fftshift(f)\n",
    "plt.imshow(np.log(np.abs(f)))\n",
    "plt.show()\n",
    "\n",
    "gray = rgb2gray(org_img)\n",
    "plt.imshow(gray, 'gray')\n",
    "# plt.show()\n",
    "f = np.fft.fft2(gray)\n",
    "f = np.fft.fftshift(f)\n",
    "plt.imshow(np.log(np.abs(f)))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af0ae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "    'Beauty':'/data/lqh/deepvideo/DVC/DVC/data/UVG/CropVideos/Beauty/Beauty/im002.png',\n",
    "    'Bosphorus':'/data/lqh/deepvideo/DVC/DVC/data/UVG/CropVideos/Bosphorus/Bosphorus/im002.png',\n",
    "    'HoneyBee':'/data/lqh/deepvideo/DVC/DVC/data/UVG/CropVideos/HoneyBee/HoneyBee/im002.png',\n",
    "    'ReadySteadyGo':'/data/lqh/deepvideo/DVC/DVC/data/UVG/CropVideos/ReadySteadyGo/ReadySteadyGo/im002.png',\n",
    "    'ShakeNDry':'/data/lqh/deepvideo/DVC/DVC/data/UVG/CropVideos/ShakeNDry/ShakeNDry/im002.png',\n",
    "    'Jockey':'/data/lqh/deepvideo/DVC/DVC/data/UVG/CropVideos/Jockey/Jockey/im002.png',\n",
    "    'YachtRide':'/data/lqh/deepvideo/DVC/DVC/data/UVG/CropVideos/YachtRide/YachtRide/im002.png'\n",
    "}\n",
    "plt.clf()\n",
    "for name, path  in d.items():\n",
    "    img = io.imread(path)\n",
    "    hist = np.histogram(img, bins=25)\n",
    "    plt.plot(hist[1][1:], hist[0], label=name)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f79779a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "org_img = io.imread('/data/lqh/deepvideo/DVC/DVC/data/UVG/CropVideos/Bosphorus/Bosphorus/im001.png')\n",
    "gray = rgb2gray(org_img)\n",
    "plt.imshow(gray, 'gray')\n",
    "plt.clf()\n",
    "hist = np.histogram(gray, bins=25)\n",
    "plt.plot(hist[1][1:], hist[0], label=name)\n",
    "\n",
    "hist = np.histogram(gray ** 0.45, bins=25)\n",
    "plt.plot(hist[1][1:], hist[0], label=name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860202bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def extract_hard_case(json_path, seq_len=4000):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    res = []\n",
    "    for k, v in data.items():\n",
    "        seqs = data[k]\n",
    "        seqs = sorted(seqs.items(), key=lambda x: x[1], reverse=True)\n",
    "        res.extend(seqs[:seq_len])\n",
    "    res_ = []\n",
    "    for k, v in res:\n",
    "        res_.append('/'.join(k.split('/')[-3:][:-1]))\n",
    "    return list(set(res_))\n",
    "\n",
    "json_path = ['output/codec-10-15_10-43-41/epoch_0000.json',\n",
    "             'output/codec-10-15_10-43-41/epoch_0001.json',\n",
    "             'output/codec-10-15_10-43-41/epoch_0002.json',\n",
    "             'output/codec-10-15_10-43-41/epoch_0003.json',\n",
    "             'output/codec-10-15_10-43-41/epoch_0004.json',\n",
    "             'output/codec-10-15_10-43-41/epoch_0005.json',\n",
    "             'output/codec-10-15_10-43-41/epoch_0006.json',\n",
    "            ]\n",
    "\n",
    "seq_len = 4000\n",
    "hard_case  =[]\n",
    "for js in json_path:\n",
    "    hard_case.extend(extract_hard_case(js, seq_len))\n",
    "hard_case = list(set(hard_case))\n",
    "np.savetxt(\"hard_case.txt\", hard_case, fmt='%s', delimiter='\\n')\n",
    "print(len(hard_case))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7856af6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "max_val = 2.2\n",
    "min_val = 1.0\n",
    "num = 7\n",
    "values = np.linspace(np.log(max_val), np.log(min_val), num)\n",
    "values = np.exp(values)\n",
    "plt.plot(list(range(len(values))), values)\n",
    "print(', '.join(list(map(str, np.around(values, 4).tolist()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d84fb24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7df0ea6aa9b76cda55610109a426938e16ac5830104bee2e540119b8c834f6d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
